
from ast import walk
import random
import gensim
import argparse
import pickle
import pandas as pd
import numpy as np
import networkx as nx
from tqdm import tqdm
from scipy.sparse import find
from _new_expr_collab_network_func import *
from datatype_change import *

def dict_of_dict_to_dataframe(node_emb_dict, emb_dim):
    total_instances = sum([len(node_emb_dict[coach].keys()) for coach in node_emb_dict.keys()])

    node_array = np.zeros((total_instances)).astype(object)
    week_array = np.zeros((total_instances)).astype(str)
    emb_array = np.zeros((total_instances, emb_dim))

    idx = 0
    for node in node_emb_dict:
        for week in node_emb_dict[node]:
            node_array[idx] = node
            week_array[idx] = week
            emb_array[idx,:] = node_emb_dict[node][week]
            idx += 1

    df_embedding = pd.DataFrame(data=emb_array, columns=["cumul_emb{}".format(i) for i in range(emb_dim)])
    df_embedding.insert(value=node_array, loc=0, column="Node") 
    df_embedding.insert(value=week_array, loc=1, column="Week") 
    return df_embedding

def get_random_walk(G, A, idx, node_map, walk_length, num_walks, bias, week_num):
    '''
    Given a graph and a node, return a sequence of nodes generated by random walks.
    Input:
      - G: NetworkX Graph. Graph of the collaboration network
      - node: NetworkX Node. The target node to start the random walks
      - walk_length: Int. The length of the node sequence generated by random walks
      - num_walks: Int. The number of random walk repeat
      - bias: Str. The type of random walk scheme. The possible options are: "unbiased",
              "hierarchy", "strength", and "recency" 
      - week_num: Int. The week sequence starting from zero. This argument is only used for
                      calculating the recency of collaborations.

    Output:
      - walk paths: List. The list of lists that contains sequences of nodes generated by random walks.
    '''
    walks = np.zeros((num_walks, walk_length+1)).astype(str)
    # repeat the random walks for the <num_walks> times
    for walk in range(num_walks):
        walks[walk,0] = node_map[idx]
        current_node = node_map[idx]
        # sample the next visiting node for the walk length
        random.seed(100)
        for step in range(walk_length):
            # sample neighborhoods based on the hierarchical probability distribution.
            if bias == "hierarchy":
                out_edges = G.out_edges(current_node)
                probability_list = [G.get_edge_data(edge[0], edge[1])['prob'] for edge in out_edges]
                next_walk = random.choices(list(out_edges), weights=probability_list, k=1)[0]
                next_visit = next_walk[1]
            # sample neighborhoods based on the collaboration frequency (sqrt transformation)
            elif bias == "strength":
                neighbors = find(A[idx,:])
                next_walk = random.choices(neighbors[1], weights=neighbors[2])
                next_visit = node_map[next_walk[0]]
            # sample neighborhoods based on the recency of collaboration (sqrt transformation)
            elif bias == "recency":
                neighbors = find(A[idx,:])
                next_walk = random.choices(neighbors[1], weights=np.sqrt(1/((week_num+1)-neighbors[2])))
                next_visit = node_map[next_walk[0]]
            # uniform sampling distribution for the unbiased random walks
            else:
                neighbors = list(nx.all_neighbors(G, current_node)) # extract neighbors
                next_visit = random.choice(neighbors) # randomly select the next visiting node

            walks[walk, step+1] = next_visit
            current_node = next_visit

    # return the list of walks
    return walks
        
def deepwalk(model, G, walk_length, num_walks, window_size, emb_size, epochs, update, bias, week_num):
    '''
    Use DeepWalk (Skip-gram) approach to learn the node embeddings for nodes in the 
    given graph <G>.
    Inputs:
      - model: Gensim Word2Vec Model. The initial model to be used for generating node embeddings. The optimizations of node embeddings
               starts from the existing model without randomly initializing parameters.
      - G: NetworkX Graph. The cumulative collaboration network to generate the node embeddings
      - walk_length: Int. The length of the node sequence generated by random walks
      - num_walks: Int. The number of random walk repeat
      - window_size: Int. The size of sliding window.
      - emb_size: Int. The size of the node embeddings (the number of vector dimensions)
      - epochs: Int. The number of iterations for optimizing embeddings
      - update: bool. Indicate whether the model should be updated based on the previous model.
                If the <update> is True, the embeddings are updated starting from the embeddings learned
                from the provided <model>.
      - bias: Str. The type of random walk scheme. The possible options are: "unbiased",
              "hierarchy", "strength", and "recency" 
      - current_year: Int. The current year (prediction year). This is used to calculate
                      the recency of collaborations.

    Output:
      - model: Gensim Word2Vec Model. Model that is trained based on the given graph <G>.
      - nodes: List. List of nodes in the graph <G>
      - embeddings: Numpy 2-D Array. Array that contains the learned embeddings of nodes. Each row is a node
                    and each column is a dimension of the embeddings.
    '''
    # list that stores all walks for all nodes
    total_walk_paths = np.zeros((nx.number_of_nodes(G), num_walks, walk_length+1)).astype(str)

    ### RANDOM WALK 
    print("Random walk...")
    nodes = G.nodes()
    idx = 0
    A = nx.adjacency_matrix(G, weight='prob')
    node_map = dict(zip(np.arange(nx.number_of_nodes(G)), nodes))
    for node in nodes:
        if G.degree(node) != 0:
            walk_paths = get_random_walk(G, A, idx, node_map, walk_length, num_walks, bias, week_num)
            total_walk_paths[idx,:,:] = walk_paths
            idx += 1

    # if this is the first model to train, initialize the model with the Word2Vec model.
    if update != True:
        # initiate word2vec model
        model = gensim.models.Word2Vec(size=emb_size, window=window_size, sg=1, hs=1, workers=500, seed=100)

    # Build vocabulary
    total_walk_paths = total_walk_paths.reshape(nx.number_of_nodes(G) * num_walks, walk_length+1).tolist()
    model.build_vocab(total_walk_paths, update=update)

    # Train
    model.train(total_walk_paths, total_examples=model.corpus_count, epochs=epochs)
    print("Training word2vec...")
    nodes = list(model.wv.vocab) # list of node names
    embeddings = model.wv.__getitem__(model.wv.vocab) # embeddings for every node

    return model, nodes, embeddings

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('-emb_size', '--emb_size', default=32, type=int, help="The number of embedding dimensions for each node")
    parser.add_argument('-window_size', '--window_size', default=3, type=int, help="The window size of Skip-gram model")
    parser.add_argument('-bias', '--bias', default='unbiased', type=str, help="The bias given to the random walk (option: unbiased, hierarchy, strength, recency)")

    args = parser.parse_args()
    emb_size = args.emb_size
    window_size = args.window_size
    bias = args.bias

    #################################################################
    # Load datasets
    employee_info_fname = "data/Company_experiment/employee_info_final.csv"
    hier_team_edgelist_fname = "data/Company_experiment/hier_team_edgelist_final.csv"
    message_collab_fname = "data/Company_experiment/message_collab_final.csv"
    dept_affil_fname = "data/Company_experiment/dept_affil_final.csv"
    paygrade_rankings_fname = "data/Company_experiment/paygrade_rankings.csv"
    
    employee_info_df = pd.read_csv(employee_info_fname)
    hier_team_edgelist = pd.read_csv(hier_team_edgelist_fname)
    message_collab_df = pd.read_csv(message_collab_fname)
    dept_affil_df = pd.read_csv(dept_affil_fname)
    paygrade_rankings_df = pd.read_csv(paygrade_rankings_fname)
    #################################################################
    # Parameter setting for node embedding 
    walk_length = 40
    num_walks = 10
    epochs = 30

    selection_prob = {'downward': 1, 'peer': 3, 'upward': 5} # relative probabilities of hierarchical ties

    # convert paygrade rankings data to a dictionary
    paygrade_m_map = list(zip(paygrade_rankings_df.paygrade_M, paygrade_rankings_df.ranking))
    paygrade_p_map = list(zip(paygrade_rankings_df.paygrade_P, paygrade_rankings_df.ranking))
    paygrade_map = dict(paygrade_m_map + paygrade_p_map)

    #################################################################
    ## Generate cumulative collaboration network embedding
    #################################################################
    weeks = sorted(message_collab_df.dt.unique())

    print("Generating corporate collaboration networks...")
    # stores graphs built by cumulative collaboration ties in each week
    cumulative_message_collab_G_dict = dict() 
    # initialize graphs
    if bias == "hierarchy":
        cumulative_message_collab_G = nx.DiGraph()
        # file = open('cumulative_message_collab_G_dict.pkl', 'rb')
        # cumulative_message_collab_G = pickle.load(file)
        # file.close()
    else:
        cumulative_message_collab_G = nx.Graph()
        file = open('cumulative_message_collab_G.pkl', 'rb')
        cumulative_message_collab_G = pickle.load(file)
        file.close()

    cumulative_node_emb_dict = dict()
    prev_week = weeks[0] 
    model = None
    prev_week = weeks[8]
    file = open('word2vec_model.pkl', 'rb')
    model = pickle.load(file)
    file.close()
    week_num = 9
    # for week in weeks[1:]:
    for week in weeks[9:]:
        print("")
        print("** Cumulative network for prediction week {}".format(week))
        collabs = message_collab_df[(message_collab_df.dt >= prev_week) & \
                                    (message_collab_df.dt < week)].reset_index(drop=True)
        print("Using collabs in week {}".format(collabs.dt.unique()))

        # unbiased random walk
        if bias == "unbiased":
            cumulative_message_collab_G = construct_cumulative_collab_network(initial_g = cumulative_message_collab_G,\
                                                df=collabs, prev_week=prev_week, bias=bias)
            num_cc = nx.number_connected_components(cumulative_message_collab_G)
        
        # hierarchy-based random walk
        elif bias == "hierarchy":
            dept_dt = dept_affil_df[dept_affil_df.dt > prev_week].dt.min()
            paygrade_df = dept_affil_df[dept_affil_df.dt == dept_dt]
            paygrade_dict = dict(zip(paygrade_df.employee_id, paygrade_df.paygrade))
            cumulative_message_collab_G = construct_cumulative_collab_network(initial_g = cumulative_message_collab_G,\
                                                df=collabs, prev_week=prev_week, bias=bias, \
                                                paygrade_dict=paygrade_dict, paygrade_map=paygrade_map,\
                                                hierarchy_prob=selection_prob)
            num_cc = nx.number_weakly_connected_components(cumulative_message_collab_G)

        elif bias == "strength":
            cumulative_message_collab_G = construct_cumulative_collab_network(initial_g = cumulative_message_collab_G,\
                                                df=collabs, prev_week=prev_week, bias=bias)
            num_cc = nx.number_connected_components(cumulative_message_collab_G)

        elif bias == "recency":
            cumulative_message_collab_G = construct_cumulative_collab_network(initial_g = cumulative_message_collab_G,\
                                                df=collabs, week_num=week_num, bias=bias)
            num_cc = nx.number_connected_components(cumulative_message_collab_G)

        prev_week = week
        print(nx.info(cumulative_message_collab_G))
        print("Number of connected componnets: {}".format(num_cc))

        # save graph
        cumulative_message_collab_G_dict[week] = cumulative_message_collab_G

        # Learn node embeddings: update the node embeddings starting from the previously learned model
        if model:
            model, cumulative_nodes, cumulative_emb = deepwalk(model, cumulative_message_collab_G, walk_length,\
                                                            num_walks, window_size, emb_size, epochs, True, bias, week_num)
        else:
            model, cumulative_nodes, cumulative_emb = deepwalk(None, cumulative_message_collab_G, walk_length,\
                                                            num_walks, window_size, emb_size, epochs, False, bias, week_num)
        week_num += 1 
        # add new embedding to the dictionary
        # Key: the employee IDs
        # value: Dictionary of embeddings for each year.
            # Key: the prediction week
            # Value: Embedding learned for predicting the week
        for idx, node in enumerate(cumulative_nodes):
            if node in cumulative_node_emb_dict:
                cumulative_node_emb_dict[node][week] = cumulative_emb[idx]
            else:
                cumulative_node_emb_dict[node] = dict()
                cumulative_node_emb_dict[node][week] = cumulative_emb[idx]

    # convert the node embedding dictionary to a dataframe
    cumulative_emb_df = dict_of_dict_to_dataframe(cumulative_node_emb_dict, emb_size)
    cumulative_emb_df.to_csv("data/Company_experiment/embeddings/cumulative_collab_G_node_embedding_size{}_{}_w{}_df2.csv".format(emb_size, bias, window_size), \
                            index=False)


